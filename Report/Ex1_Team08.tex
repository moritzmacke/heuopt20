\documentclass{scrartcl}
\usepackage[shortlabels]{enumitem}
\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}

\begin{document}
	\begin{enumerate}
		\item ...
		\item Given is a (possibly not complete) graph $G=(V,E)$ and edge weigths $d_{i,j}$ for $(i,j) \in E$. To make the graph complete, an additional set of edges $E'$ is defined, consisting of all edges that are not alread in $E$, and we set $d_{i,j} = M$ for $(i,j) \in E'$. We want to calculate a value for $M$ so that any solution that requires traversing an edge from $E'$ is worse than all solutions that do not.
		
		A solution must visit $|V| + 1$ vertices (every node once, the starting node twice), so it always consists of $|V|$ distinct edges. A definite upper bound on the objective value of a valid solution can therefore be obtained via $E_{|V|}^+$ and $E_{|V|}^-$, which are the $|V|$ edges with the largest and smallest weights, respectively.
		\[ U(G) = \max\left( \left| \sum_{(i,j) \in E_{|V|}^+}{d_{i,j}} \right|, \left| \sum_{(i,j) \in E_{|V|}^-}{d_{i,j}} \right| \right) \]
		Note that if there are less than $|V|$ edges with a particular sign, edges with the opposing sign are also included in $E_{|V|}^+$ and $E_{|V|}^-$, since they will only bring the sum closer to 0 and therefore make the upper bound tighter.
		
		To prove this is an upper bound, we examine 3 cases:
		\begin{itemize}
			\item $\sum_{(i,j) \in E_{|V|}^+}{d_{i,j}} \ge 0$ and $\sum_{(i,j) \in E_{|V|}^-}{d_{i,j}} < 0$: Increasing the sum of $E_{|V|}^+$ further would require replacing an edge $(i_0, j_0)$ with a different, unused edge $(i_1,j_1)$ so that either $d_{i_1,j_1} > d_{i_0,j_0}$ or $d_{i_1,j_1} < -2\sum_{(i,j) \in E_{|V|}^+}{d_{i,j}} + d_{i_0,j_0}$ (so the sum flips to negative and increases in absolute value). The first option is impossible since all edges with the greatest weights have been used, and in the second, $(i_1,j_1)$ or an edge with even smaller weight must be included in $E_{|V|}^-$ along with all other smallest possible edges, meaning both the original $E_{|V|}^+$ sum and our new sum cannot be larger than the $E_{|V|}^-$ sum, which is the actual upper bound due to the $\max$ function. The same idea applies to decreasing the sum of $E_{|V|}^-$.
			\item $\sum_{(i,j) \in E_{|V|}^+}{d_{i,j}} \ge 0$ and $\sum_{(i,j) \in E_{|V|}^-}{d_{i,j}} \ge 0$: In this case the sum of $E_{|V|}^+$ is automatically chosen as the upper bound and there is actually no way to sum $|V|$ edges up to a negative value, so we could only increase the objective value by replacing an edge from $E_{|V|}^+$ with an edge that has a greater weight. As stated before, this is clearly not possible.
			\item $\sum_{(i,j) \in E_{|V|}^+}{d_{i,j}} < 0$ and $\sum_{(i,j) \in E_{|V|}^-}{d_{i,j}} < 0$: See previous case.
		\end{itemize}
		No other cases are possible because $\sum_{(i,j) \in E_{|V|}^+}{d_{i,j}} \ge \sum_{(i,j) \in E_{|V|}^-}{d_{i,j}}$.
		
		Meanwhile, the lower bound on the objective value of a solution containing an edge with weight $M > 0$ can be found by assuming all other edges have the smallest weights possible.
		\[ L(G,M) = M + \sum_{(i,j) \in E_{|V|-1}^-}{d_{i,j}} \]
		
		Now we just need to find $M$ so that the disequality $L(G,M) > U(G)$ is satisfied:
		\begin{align*}
			M + \sum_{(i,j) \in E_{|V|-1}^-}{d_{i,j}} &> \max\left( \left| \sum_{(i,j) \in E_{|V|}^+}{d_{i,j}} \right|, \left| \sum_{(i,j) \in E_{|V|}^-}{d_{i,j}} \right| \right)\\
			M &> \max\left( \left| \sum_{(i,j) \in E_{|V|}^+}{d_{i,j}} \right|, \left| \sum_{(i,j) \in E_{|V|}^-}{d_{i,j}} \right| \right) - \sum_{(i,j) \in E_{|V|-1}^-}{d_{i,j}}
		\end{align*}
		\item ...
		\item ...
		\item ...
		\item ...
		\item ...
		\item ...
		\item ...
		\item We implemented \textbf{Simulated Annealing} as the metaheuristic. The pymhlib already contains a SA class, which can be instantiated with our own initial solution, construction heuristic, and neighborhood exploration functions. With those elements, the algorithm then does the following:
		\begin{enumerate}[\arabic*.]
			\item The construction heuristic(s) is applied, and the initial solution we passed in is replaced if this results in a better solution.
			\item A neighborhood move is selected at random and delta evaluated.
			\item The delta value is used to decide the acceptance of the move via the metropolis criterion: Improvements are always accepted, otherwise the probability is $e^{-\frac{|\delta|}{t}}$, where $\delta$ is the delta value and $t$ the current temperature.
			\item If the move was accepted, it is applied to the solution and the objective value is updated according to the delta evaluation.
			\item Steps 2-4 repeat for a fixed number of iterations.
			\item The temperature is reduced via geometric cooling.
			\item Step 2-6 repeat until the algorithm hits an iteration or time limit.
		\end{enumerate}
	
		However, the built-in SA class initially produced only very bad results, which was due to the fact that the check whether a solution was an improvement was done by comparing the delta value to 0 as an objective function value. For our solution representation, these comparison functions compare the absolute value of the actually stored objective value, so a comparison to 0 will obviously always return false, and improvements will not be accepted. This was fixed by extending the SA class and overriding the metropolis criterion with a variation that checks for improvements with a dedicated function \verb|is_delta_improvement|.
	\end{enumerate}
\end{document}